{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Dataloader\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_matrix(arr, n_cols):\n",
    "    n_rows = len(arr) // n_cols\n",
    "    return np.array(arr).reshape(n_rows, n_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp  = MLPClassifier(hidden_layer_sizes=(15,),activation='relu', solver='adam', random_state=1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris-setosa + Iris-versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching iris dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "Dataset:  iris\n",
      "X_train:  [[ 0.35866332 -0.62068428  1.1364712   0.91401319]\n",
      " [ 0.20204178 -0.19845007  0.85894465  0.91401319]]\n",
      "y_train:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "X_test:  [[ 0.82852793 -0.83180138  1.55276101  1.44644806]\n",
      " [ 0.04542025 -1.67626978  0.78956302  0.91401319]]\n",
      "y_test:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "(80, 4) (80, 2) (20, 4) (20, 2)\n",
      "Exporting iris to python... \n",
      "Training time: 0.16049957275390625 seconds\n"
     ]
    }
   ],
   "source": [
    "loader = Dataloader(None, None, 'iris', 'Iris-setosa', 'Iris-versicolor')\n",
    "print(f\"Exporting iris to python... \")\n",
    "data = loader.export_to_python()\n",
    "X_shape = loader.X_train_shape\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "y = array_to_matrix(y_train,2)\n",
    "X = array_to_matrix(X_train,X_shape[1])\n",
    "y = np.array([np.argmax(np.append(i,0)) for i in y])\n",
    "start_time = time.time()\n",
    "mlp.fit(X, y) \n",
    "training_time = time.time() - start_time\n",
    "print(\"Training time:\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "(4, 15)\n",
      "Layer 1:\n",
      "(15, 1)\n",
      "[0.7950032333410059, 0.7877308718272474, 0.7805369350343137, 0.7734136081040199, 0.7663475576233808, 0.7593370722084474, 0.752376475210024, 0.7454692383591178, 0.738607162831884, 0.7317897057911993, 0.7250279256999865, 0.7183193248145201, 0.7116623014792461, 0.7050605721486891, 0.6985143582305249, 0.6920255151857528, 0.6855933834120241, 0.6792333408384854, 0.6729093850892429, 0.6666369062015118, 0.6604150628802969, 0.6542572293403621, 0.6481641437794897, 0.6421326896895271, 0.6361693870970924, 0.6302706333123809, 0.6244305311219366, 0.6186469162087774, 0.6129213634644386, 0.6072556943508076, 0.601651828665329, 0.5961040269223397, 0.5905970934314946, 0.5851385170058845, 0.5797277459146531, 0.5743753770624115, 0.5690817685509738, 0.5638525050938541, 0.5586660095897161, 0.5535358568767763, 0.5484611817322077, 0.5434296653900464, 0.5384474690233853, 0.5335168956087245, 0.5286299827432102, 0.5237900709596915, 0.5190066094766498, 0.5142657203640275, 0.5095673643097203, 0.5049085105685075, 0.5002686398029352, 0.4956703812868021, 0.49111263300347396, 0.4865972114579348, 0.48212084194869653, 0.47768141555660015, 0.47328411716398255, 0.468932485378126, 0.46461278060482697, 0.46032637947118704, 0.4560764147524403, 0.45186586267443796, 0.4476970423007053, 0.44356543010098715, 0.43947897798402646, 0.43542680998172456, 0.43140810615633746, 0.4274238880343321, 0.42347633348694114, 0.4195652323984276, 0.4156905579605955, 0.4118515506904553, 0.4080487408329894, 0.40427934305177116, 0.40053951642268226, 0.39683661397029324, 0.3931572822514815, 0.38950489049709686, 0.38588218113486755, 0.382280516065012, 0.3787096817426658, 0.3751694536716131, 0.3716609714792733, 0.36818407277405824, 0.3647387698095452, 0.3613281414535614, 0.35795180395915915, 0.35460840131677135, 0.3512972027758466, 0.3480189968159199, 0.34477254105150756, 0.34155729347446234, 0.33837303928999257, 0.3352196430654084, 0.3320869051164031, 0.3289839218068598, 0.32591056800290946, 0.32286670501830733, 0.31985382083658326, 0.3168703801247429, 0.31391538077082337, 0.31098707305901085, 0.3080849425506264, 0.3052132381729089, 0.3023696710897916, 0.2995540308940496, 0.2967653766330542, 0.29400411313514907, 0.2912701472554, 0.2885645413857368, 0.2858923692766179, 0.2832500998699577, 0.2806339758958193, 0.27804452639658717, 0.2754840374695866, 0.27295241596048225, 0.2704484417969999, 0.267970237441602, 0.26551753539142886, 0.263090072049246, 0.2606879778328913, 0.2583140414954294, 0.2559639933561612, 0.2536368828368378, 0.2513312353963534, 0.24904759069989968, 0.24678573093750886, 0.2445445169024713, 0.24232694560419973, 0.2401328766085427, 0.2379633870318562, 0.23581452972699452, 0.233687603381557, 0.23158017906358458, 0.22948998173835947, 0.22742083178216016, 0.22537382268299527, 0.22334781369697038, 0.2213417342503224, 0.21935379166951421, 0.21738286032667128, 0.2154318202604036, 0.2135013923482086, 0.2115921205393095, 0.20969982409481483, 0.2078249374803493, 0.2059682115437021, 0.20413012910345438, 0.20230991061320966, 0.20050797299963144, 0.19872346805719504, 0.19695592989224095, 0.19520675098980428, 0.1934766198376424, 0.19176432707374214, 0.1900686886807847, 0.18838868602873632, 0.18672599070271945, 0.18508078991409466, 0.18345297634029117, 0.18184239709727476, 0.18024864371029847, 0.17867157745064408, 0.1771098105686997, 0.1755617869173978, 0.17402978111002307, 0.17251393419688693, 0.171014427398422, 0.1695306391252277, 0.1680624383073982, 0.16660969058976627, 0.16517225884886994, 0.16375000365342632, 0.1623427836744295, 0.160950456050324, 0.15957289080646117, 0.15821038304991086, 0.15686305101244524, 0.15553118189998533, 0.15421354282574273, 0.1529087134237818, 0.15161688786549896, 0.15033871492134446, 0.14907406524494804, 0.14782333483246032, 0.14658577244703697, 0.14536150233316014, 0.1441506824236395, 0.142952739786931, 0.1417672356083989, 0.1405943313652097, 0.13943355372266175, 0.13828440237633538, 0.13714736433116556, 0.13602225791461192, 0.13490919562054174, 0.13380792510985232, 0.13271836284096328, 0.1316409250790952, 0.1305748830962271, 0.1295201068316747, 0.128476467253572, 0.127444499871302, 0.12642359235543046, 0.1254134826335903, 0.12441397300004131, 0.12342346839465726, 0.1224430248797916, 0.12147275219175172, 0.12051254558492738, 0.11956256878632454, 0.11862211941821035, 0.11769111800642267, 0.11676849034382439, 0.11585470051038646, 0.11495040188304753, 0.11405564953508433, 0.11317003477903445, 0.11229346474473115, 0.11142584962831577, 0.11056709880750973, 0.1097171413827395, 0.10887587977362723, 0.10804341221087244, 0.10722013039612781, 0.10640530313453286, 0.10559891211054366, 0.10480119640351789, 0.10401166795924513, 0.10323022637233806, 0.10245683655717222, 0.10169149196625474, 0.10093395586156403, 0.10018413171284894, 0.09944155236281255, 0.09870649293461238, 0.09797921499232883, 0.09725888428407795, 0.09654464752328851, 0.09583763623389469, 0.0951376320084554, 0.09444456266233037, 0.09375818228595548, 0.09307794839683872, 0.09240438020580234, 0.09173741403402949, 0.09107698537809833, 0.09042307592139288, 0.08977589892278556, 0.08913508177859175, 0.0885005548310264, 0.08787224868900602, 0.08725009429031587, 0.08663402295626069, 0.08602396643957554, 0.08541985696629455, 0.08482170339032163, 0.08422951996951807, 0.08364309757174536, 0.08306247449131862, 0.08248759618314015, 0.0819182904537699, 0.08135449019405558, 0.08079612931022329, 0.08024314269737479, 0.07969546621453197, 0.07915261980890664, 0.07861386362344917, 0.0780803305385046, 0.07755178865224366, 0.07702819041555549, 0.07650948781016208, 0.075995632463361, 0.07548665451390238, 0.07498258444862259, 0.07448322808675956, 0.0739885343811066, 0.07349845257401237, 0.0730129322316068, 0.07253199101042454, 0.07205560082928696, 0.07158363285262415, 0.07111603689145514, 0.07065260050120285, 0.07019331753382584, 0.0697379447768994, 0.0692867110977453, 0.06883957494993698, 0.068396494739515, 0.06795742888399602, 0.06752233586478859, 0.06709117427367399, 0.06666390285394616, 0.06624048053674741, 0.06582086647308362, 0.06540502006195499, 0.06499290204681625, 0.06458475574403533, 0.06418027564755276, 0.06377941937863464, 0.06338214555892296, 0.06298841339973171, 0.06259818268918164, 0.06221141377994631, 0.061828067577557475, 0.061448105529224514, 0.061071412017579665, 0.06069793256278787, 0.06032750519346998, 0.059960308112478135, 0.05959629593492728, 0.0592354310020529, 0.05887768155271467, 0.05852301592685007, 0.05817140259440987, 0.05782281018092046, 0.05747720749000516, 0.05713456352316506, 0.05679484749709175, 0.05645802885875848, 0.05612407729851341, 0.0557928654866283, 0.05546315639663126, 0.05513636172655524, 0.05481231263701656, 0.05449093091609558, 0.0541721959755206, 0.053856179091071654, 0.0535429688921968, 0.053232346358003446, 0.052924281711590124, 0.052618765400751236, 0.052315838646890434, 0.052015458609330746, 0.05171754203077576, 0.05142207610469226, 0.051129041260151095, 0.05083841300672145, 0.05055016701936984, 0.050264279150799965, 0.04998072544213472, 0.049699482132112914, 0.0494205256649548, 0.049143832697036476, 0.048869077949768344, 0.04859624401345373, 0.04832555686998432, 0.04805700049508214, 0.04779055843336666, 0.04752621386569259, 0.0472639496695598, 0.04700374847326905, 0.04674559270443394, 0.04648943110317211, 0.04623523116544223, 0.045983058965368744, 0.045732870153397734, 0.045484661478635544, 0.045238470852883274, 0.04499420659470554, 0.04475185001772543, 0.04451138256472236, 0.04427278581589633, 0.04403604149601393, 0.0438011314805513, 0.04356803780093726, 0.043336742648990415, 0.04310722838063398, 0.04287947751896487, 0.04265347275674448, 0.04242934970393944, 0.04220717928392176, 0.041986718879388145, 0.041767947523540155, 0.04155085122567103, 0.04133541141083625, 0.041121609923814396, 0.040909429004378065, 0.040698851264778436, 0.04048985966922722, 0.04028246231826768, 0.040076629092666384, 0.039872334910111204, 0.039669563660074576, 0.03946829953734633, 0.03926852702683468, 0.03907023088966399, 0.03887339615044445, 0.03867800808560021, 0.03848405221265508, 0.03829151428038374, 0.03810038025974588, 0.03791063633552906, 0.037722268898632984, 0.03753526453893475, 0.037349610038680536, 0.037165309868931504, 0.03698236473434207, 0.036800733820418985, 0.03662040422669795, 0.03644136325829513, 0.03626358641944672, 0.03608706721092108, 0.03591179844584462, 0.035737768394007836, 0.03556496547147952, 0.0353933782374794, 0.03522299539140949, 0.03505380577003055, 0.03488579834477136, 0.034718962219160356, 0.03455328662636925, 0.034388760926859996, 0.03422537460612691, 0.03406311727252678, 0.03390197865519021, 0.03374194860200821, 0.03358301707768872, 0.033425198710966025, 0.03326860912441487, 0.03311309922099014, 0.03295865818534318, 0.03280527221013998, 0.032652933751277174, 0.03250163276177143, 0.03235135929817064, 0.03220210359521115, 0.03205387803072174, 0.031906661020057514, 0.03176043521199567, 0.031615191242300306, 0.031470919910227275, 0.03132761217053302, 0.03118525912616566, 0.031043852021572515, 0.030903382236565136, 0.030763841280688257, 0.030625220788044742, 0.03048751251253316, 0.03035070832345873, 0.03021480020148249, 0.03007979668395729, 0.029945682187380158, 0.029812441773941257, 0.02968006761788202, 0.029548552011167906, 0.029417887358649048, 0.029288066173614936, 0.029159081073705427, 0.029030924777144074, 0.028903590099262976, 0.028777069949291512, 0.028651357327383935, 0.028526445321863398, 0.028402327106662316, 0.028278995938940166, 0.02815644515686312, 0.028034668177529788, 0.027913658495030143, 0.027793409678625335, 0.02767391537103746, 0.027555169286839536, 0.02743716521093642, 0.027319896997129216, 0.027203358566755244, 0.027087548083932704, 0.026972468525212047, 0.026858101764338498, 0.026744441846876448, 0.02663148289396437, 0.026519219099905005, 0.026407644729935054, 0.026296754118157066, 0.026186541665618394, 0.02607700183852321, 0.025968129166564942, 0.025859918241368268, 0.025752363715030212, 0.025645460298751414, 0.025539221275176594, 0.025433641487158563, 0.025328699612192834, 0.025224390259832453, 0.025120708126468255, 0.025017261218861896, 0.024912647236929663, 0.024808337846493663, 0.024704363727553712, 0.02460075208239012, 0.024497526977744436, 0.024394709654268185, 0.024292318806266443, 0.02419037083450213, 0.02408888007458377, 0.02398785900323644, 0.02388732586678692, 0.023787299644251823, 0.023687772759633432, 0.023588751731730636, 0.023490242008623704, 0.023392248076936246, 0.023294773560370956, 0.023197821308557073, 0.023101393477149436, 0.023005491600029498, 0.022910116654377317, 0.022815269119310054, 0.022720949028715052]\n",
      "508\n"
     ]
    }
   ],
   "source": [
    "weights = mlp.coefs_\n",
    "for i, layer in enumerate(weights):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(layer.shape)\n",
    "# print(weights)\n",
    "loss = mlp.loss_curve_ \n",
    "print(loss)\n",
    "print(len(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_test = array_to_matrix(data[2],X_shape[1])\n",
    "y_test = array_to_matrix(data[3],2)\n",
    "y_test = np.array([np.argmax(np.append(i,0)) for i in y_test])\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82852793 -0.83180138  1.55276101  1.44644806]\n",
      " [ 0.04542025 -1.67626978  0.78956302  0.91401319]\n",
      " [ 0.6719064   0.22378413  1.3446161   1.80140464]\n",
      " [-1.05093052 -0.19845007 -1.01435952 -0.8607697 ]\n",
      " [-0.5810659   1.49048673 -0.66745134 -0.68329141]\n",
      " [-0.5810659   0.64601833 -0.94497788 -1.03824799]\n",
      " [-1.36417359  1.06825253 -1.29188606 -1.03824799]\n",
      " [ 0.04542025 -1.46515268  0.65079974  0.55905661]\n",
      " [-0.11120129  1.27936963 -0.94497788 -1.03824799]\n",
      " [-0.5810659   0.85713543 -1.01435952 -1.03824799]\n",
      " [ 0.35866332  1.49048673 -0.80621461 -0.8607697 ]\n",
      " [-1.05093052  0.01266703 -0.87559625 -1.03824799]\n",
      " [ 0.98514947 -0.62068428  1.27523447  0.7365349 ]\n",
      " [ 0.04542025  2.33495513 -1.01435952 -1.03824799]\n",
      " [ 0.04542025 -1.04291848  1.06708956  0.7365349 ]\n",
      " [-0.73768744  1.06825253 -1.01435952 -1.03824799]\n",
      " [ 2.08150023 -0.62068428  1.3446161   1.09149148]\n",
      " [ 1.92487869 -0.19845007  1.48337938  1.62392635]\n",
      " [-1.05093052 -0.19845007 -1.01435952 -1.21572628]\n",
      " [-0.11120129  0.64601833 -0.94497788 -0.68329141]]\n",
      "[1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris-virginica Iris-versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching iris dataset\n",
      "(150, 4) (150,)\n",
      "Dataset:  iris\n",
      "X_train:  [[ 2.02877297  0.38660992  2.06223168  1.00321947]\n",
      " [-0.39726347  0.38660992 -0.12904165  0.29339437]]\n",
      "y_train:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "X_test:  [[ 0.05761837 -0.21746808  0.23617057 -0.41643072]\n",
      " [ 0.05761837  0.08457092  0.84485761  0.29339437]]\n",
      "y_test:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "(80, 4) (80, 2) (20, 4) (20, 2)\n",
      "Exporting iris to python... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = Dataloader(None, None, 'iris', 'Iris-virginica', 'Iris-versicolor')\n",
    "print(f\"Exporting iris to python... \")\n",
    "data = loader.export_to_python()\n",
    "X_shape = loader.X_train_shape\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "y = array_to_matrix(y_train,2)\n",
    "X = array_to_matrix(X_train,X_shape[1])\n",
    "y = np.array([np.argmax(np.append(i,0)) for i in y])\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 1 1 0 0 1 0 1 1 1 0 1 1 0 0]\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "X_test = array_to_matrix(data[2],X_shape[1])\n",
    "y_test = array_to_matrix(data[3],2)\n",
    "y_test = np.array([np.argmax(np.append(i,0)) for i in y_test])\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7772026975891775, 0.7717873886590556, 0.7664034158994707, 0.7610611277698213, 0.7557614629711835, 0.7505109632681383, 0.745338428241672, 0.7402152056493523, 0.7351426235586664, 0.73012509583612, 0.7251640503193847, 0.7202645295114466, 0.715418681399614, 0.7106212327203926, 0.7058592558472349, 0.7011322603582524, 0.6964465224793619, 0.6918089716294807, 0.6872397437736146, 0.6827213384031978, 0.6782519900463084, 0.6738213730431759, 0.6694282148163673, 0.665077829523303, 0.6607731499374007, 0.6565158608968925, 0.6523093352556554, 0.6481471292123188, 0.6440292896628205, 0.6399519333493988, 0.6359241161850772, 0.6319431886841701, 0.6280105198516218, 0.6241168093769462, 0.6202686981714384, 0.6164722489874487, 0.6127282565018409, 0.6090401453460095, 0.6053953302981218, 0.6017911204120885, 0.5982262934380546, 0.594702434939164, 0.5912198096376811, 0.5877784147786254, 0.5843804470176073, 0.5810249191103427, 0.5777135506920306, 0.5744426564292232, 0.5712123058906552, 0.5680215043946323, 0.5648674181579388, 0.5617494502474307, 0.5586670451578137, 0.5556231930066109, 0.5526204137252038, 0.5496636573773763, 0.5467417846786162, 0.5438532367657128, 0.5410008961865618, 0.5381813636257462, 0.535394052152551, 0.5326393642665974, 0.5299165247044502, 0.5272239314857219, 0.5245583179616888, 0.521922033613933, 0.5193131442711402, 0.5167329048389385, 0.5141832896349713, 0.5116609107748009, 0.5091653107103925, 0.5066960358055781, 0.5042543816123398, 0.5018374589569904, 0.49944124279494895, 0.49706635710283625, 0.4947110635293336, 0.4923751780786664, 0.49006021551416695, 0.48776809999243, 0.4854985919652501, 0.4832517871688795, 0.48103012433436876, 0.4788290957570032, 0.47664834528176875, 0.47448778528662333, 0.4723472271002292, 0.47022478351620195, 0.46812132862000777, 0.4660400930698472, 0.4639797574938724, 0.4619372898576456, 0.4599110011520849, 0.4579038440498683, 0.45591256155763177, 0.4539373857670184, 0.4519783276688595, 0.4500364175832202, 0.4481114652096263, 0.4462019034725432, 0.44430877655614465, 0.4424316226324764, 0.44056909783703135, 0.43872091547380265, 0.4368867984904624, 0.4350598105549728, 0.43324490696584217, 0.4314427186486098, 0.4296521163836581, 0.42787055183724665, 0.42609999131584947, 0.42434094206817924, 0.4225936509383287, 0.42085807183087176, 0.41913388498350745, 0.41742060975508244, 0.4157145688898032, 0.4140151454024224, 0.41232314690290905, 0.41063741172931145, 0.408958182376051, 0.4072860712597772, 0.4056197026346381, 0.40395863029031737, 0.402306021536574, 0.40066290043122416, 0.3990278467552606, 0.3973993471685926, 0.3957780460092403, 0.3941675766570164, 0.3925616007063444, 0.3909608618141908, 0.3893679727523849, 0.38778422585834765, 0.3862099581237856, 0.38464370361390965, 0.3830854499265957, 0.38153517576711926, 0.3799928520775627, 0.3784584430473687, 0.3769320561108763, 0.375415505178994, 0.3739050797264498, 0.37239877705457697, 0.37089963064907566, 0.36940241352964837, 0.3679108976314545, 0.3664259797462544, 0.36494858921138107, 0.3634809803963432, 0.36201726936978895, 0.3605551654630176, 0.3591035186870233, 0.35766033810997, 0.35622299953552733, 0.3547921995819677, 0.35336693916170325, 0.3519476648433023, 0.3505355529050086, 0.34912977575916815, 0.3477303119655618, 0.346337136466204, 0.34494958001010556, 0.343568188503097, 0.34219081785475625, 0.34081265353207396, 0.3394385175314016, 0.33806960501891564, 0.33670597976376015, 0.3353473687595297, 0.3339928227105135, 0.3326428843863444, 0.3312983501741504, 0.3299588907109702, 0.32862476537014035, 0.32729633666952856, 0.3259764636919022, 0.32466217104264333, 0.3233534767162859, 0.32205057932522474, 0.32075568914784863, 0.31946755359137124, 0.3181851287787465, 0.31690864893796716, 0.31563727772646905, 0.3143712141468032, 0.313110607823869, 0.3118560914308718, 0.3106081578584429, 0.30936569815916404, 0.30812724832472865, 0.30689299914335944, 0.30566405200684305, 0.30444017700439824, 0.3032206737456027, 0.30200595831176075, 0.30079620116137407, 0.29959234383881694, 0.2983930873452032, 0.2971985094618182, 0.29600413730810626, 0.2948074404331537, 0.29361300638347204, 0.29242215614878947, 0.2912349130766835, 0.2900534868881576, 0.28887605240671405, 0.2877011193304338, 0.2865301207478297, 0.28536314728062384, 0.28420027192353886, 0.28304155840936374, 0.2818870621838317, 0.28073683128090493, 0.2795909555851718, 0.2784494831158462, 0.2773121566816763, 0.2761823409735303, 0.2750578187222165, 0.2739376094590471, 0.2728217439618528, 0.2717113571701478, 0.27060408230987393, 0.2695005734644757, 0.26840141462119654, 0.26730663153931533, 0.266215762535918, 0.2651270553841669, 0.2640427800042418, 0.2629633942614476, 0.26188833478493506, 0.26081762543607223, 0.25975140726511026, 0.25868961482377967, 0.2576322453169498, 0.25657930338533824, 0.2555307913863093, 0.2544867096486926, 0.2534469344186058, 0.25240915784185075, 0.25137549677492016, 0.250345981469718, 0.24932157157814458, 0.24830182690560815, 0.2472863562360129, 0.24627549681773148, 0.2452697322840956, 0.2442682054557051, 0.24326728371655687, 0.24226891448006221, 0.24127443848858987, 0.240283900191376, 0.23929733796023692, 0.23831478471386097, 0.2373371977770628, 0.2363637769401273, 0.23539448007198865, 0.23442931688583477, 0.2334684303190277, 0.23251102716077696, 0.23155516921842856, 0.23060315293577474, 0.2296544224718305, 0.22870937324219348, 0.22776824042846755, 0.2268303046777787, 0.22589589046196465, 0.2249652997888772, 0.22403857543125047, 0.22311575429970204, 0.22219686792285387, 0.2212816491888902, 0.22037036820950498, 0.2194630487241284, 0.21856035610950544, 0.21766090108165298, 0.21675245355670586, 0.21584785186064046, 0.21494735890043065, 0.21404982894957664, 0.2131553875717569, 0.21226414561318396, 0.21137771020994417, 0.21049766784347584, 0.2096211440024387, 0.20874770001599896, 0.2078779634474804, 0.207011972508569, 0.20614970811716957, 0.20529179209368853, 0.20443857602520737, 0.20358915473428912, 0.20274352592605874, 0.20190183850444116, 0.2010638927323086, 0.20022924809347575, 0.19939710560201976, 0.19856800531028793, 0.19774201557938145, 0.1969194037231428, 0.19610078114535315, 0.19528592462527114, 0.19447807863039196, 0.19367583357233273, 0.19287767912097536, 0.19208390606649728, 0.1912943298613746, 0.19050911334023926, 0.18972818427482818, 0.18895096665593739, 0.18817824154261062, 0.1874088336889517, 0.18663835817609686, 0.18587130531544696, 0.1851077010526275, 0.18434758287679578, 0.18359346480924849, 0.18284367121619127, 0.18209763231523046, 0.18135543265324663, 0.18061695692586088, 0.17988257295842236, 0.17915266095480145, 0.17842658679203072, 0.17770424415621042, 0.17698562531115197, 0.17627071091803542, 0.1755594819335473, 0.17485191958350388, 0.17414800533938665, 0.17344772089741675, 0.17275265902605988, 0.17206220341932574, 0.1713753768403443, 0.17069214733801089, 0.17001248980272826, 0.1693352625025943, 0.1686603519819288, 0.16798882202592272, 0.16732047947171474, 0.16665535179365923, 0.16599352540220802, 0.16533502228192268, 0.16467974848974762, 0.1640277205733465, 0.16337895134568878, 0.1627334501689039, 0.16209131556755152, 0.1614525125365009, 0.1608170015685983, 0.16018477954890054, 0.159555841374174, 0.15893020677115738, 0.15830793177161737, 0.1576889242141939, 0.15707317181469282, 0.15646066136638745, 0.15585137887241815, 0.15524530966569342, 0.15464253508409348, 0.1540431120426711, 0.15344711855833823, 0.15285429121742966, 0.15226477041070632, 0.15167846067206492, 0.151095287753429, 0.1505152405943737, 0.14993828816847588, 0.1493644398337828, 0.14879349023793323, 0.14822552859441035, 0.14766061323034854, 0.1470986914237931, 0.14653971053247677, 0.1459837223745643, 0.14542917058029534, 0.1448744676586987, 0.144322509716306, 0.14377540815123477, 0.1432315377082694, 0.1426905734465067, 0.14215222599034244, 0.14161646050029028, 0.14108024784010792, 0.14054578071581195, 0.1400136478630818, 0.13948414838411666, 0.1389571625228798, 0.13843286535366586, 0.1379111410807114, 0.13739203512311296, 0.13687558171109065, 0.1363617746188802, 0.13585093174463878, 0.13534281559933278, 0.1348375466510032, 0.13433511517976615, 0.13383540939285699, 0.13333844316128365, 0.13284415998950241, 0.13235286633738877, 0.13186440348483272, 0.13137907487744718, 0.1308972046974894, 0.13041808559645593, 0.12994184562986824, 0.12946826908657605, 0.12899730584352648, 0.12852894720231936, 0.12806315261837434, 0.12759991895378678, 0.12713920230625841, 0.12668101959146114, 0.12622612891794904, 0.12577362133692657, 0.1253233618199552, 0.12487549539346625, 0.1244300262924651, 0.12398678284682521, 0.12354582180413103, 0.12310735801680968, 0.12267190712492473, 0.12223873654950448, 0.1218079852350851, 0.1213814154752511, 0.12095722609235135, 0.12053532981794698, 0.12011569175273151, 0.11969835351606892, 0.1192808433633427, 0.11886506322098445, 0.11845174096228839, 0.11804038565408773, 0.11763152618050161, 0.11722458791857462, 0.11681948191163893, 0.11641661541102151, 0.1160158949241445, 0.11561720037580163, 0.11522045307232791, 0.11482564608519139, 0.11443277971366857, 0.11404185306456271, 0.11365286417755312, 0.11326602932385463, 0.11288122842636847, 0.11249829793229218, 0.11211716507267147, 0.11173816638818937, 0.11136149486245457, 0.11098668016322774, 0.11061371925963417, 0.11024260835853554, 0.1098733985286641, 0.10950600005200542, 0.10913797906725904, 0.10877160074306684, 0.10840677250080319, 0.10804384632179036, 0.10768259838666565, 0.10732287988615456, 0.1069643443242694, 0.10660741380022139, 0.10625208415498125, 0.10589859066252197, 0.10554692161555404, 0.10519679406914015, 0.10484816917808118, 0.10450143030223222, 0.10415637745416204, 0.10381295836245356, 0.10347118129209151, 0.10313105638622411, 0.1027925661684272, 0.10245592946849709, 0.10212085972229441, 0.10178764851604208, 0.10145622934423341, 0.10112640002767832, 0.1007981630986033, 0.10047170199637898, 0.1001468685208364, 0.09982359766690917, 0.09950198688919125, 0.09918205436820518, 0.09886365345886586, 0.09854688294420864, 0.09823199519671984, 0.09791856300648975, 0.09760663761907344, 0.09729615649027575, 0.09698752298917482, 0.0966804696768445, 0.09637493923023742, 0.09607085051227131, 0.09576820478935626, 0.09546706245707981, 0.09516778843672197, 0.09486999733142559, 0.09457359345936828, 0.09427854751046415, 0.09398489638948641, 0.09369300184897404, 0.0934027753750174, 0.09311400462921662, 0.09282658844069941, 0.09254058133832331, 0.09225596265911308, 0.09197310109512795, 0.09169162243886, 0.09141148158469414, 0.09113264154383158, 0.09085513919324248, 0.09057907733060912, 0.09030450206343008, 0.09003121914110324, 0.0897592112346581, 0.08948851774054646, 0.08921919221933038, 0.08895135393089436, 0.08868473855721132, 0.08841937736670653, 0.08815542709380852, 0.08789279562961234, 0.08763157806411925, 0.08737169735463167, 0.0871130779623671, 0.08685575099711088, 0.086599644429949, 0.08634478124014375, 0.08609117561174028, 0.08583880123564108, 0.08558759318567331, 0.08533749971474532, 0.08508860662196172, 0.08484089516215566, 0.08459447132652897, 0.08434912462627762, 0.08410518821698514, 0.0838625970286212, 0.08362175216120217, 0.08338261800225323, 0.08314468205099518, 0.08290795391666314, 0.08267234040743504, 0.08243781788113952, 0.08220444206756096, 0.08197218885357185, 0.08174120297335757, 0.08151113347985393, 0.0812820586377347, 0.08105425615844859, 0.08082753713230621, 0.08060185017130332, 0.08037723432374624, 0.08015366994574427, 0.0799311744514815, 0.07970972219466335, 0.07948936241022612, 0.07926996394262549, 0.07905166399392659, 0.07883442530678575, 0.07861817703951529, 0.07840297802763768, 0.07818876971644548, 0.07797556634838432, 0.07776333999145427, 0.07755210835215998, 0.07734194327195382, 0.07713264239066248, 0.07692441077719334, 0.07671716011296757, 0.07651085675578498, 0.07630549122833065, 0.07610110602978712, 0.07589764880719074, 0.075695152652611, 0.07549356758492319, 0.0752928880834115, 0.07509332111413762, 0.07489456615974127, 0.07469659396036199, 0.07449967200720271, 0.07430363545122544, 0.07410852865732642, 0.0739142988553772, 0.0737209289373258, 0.07352852644945097, 0.07333690467375714, 0.07314622095150182, 0.07295642573132231, 0.07276746846379979, 0.07257934689634896, 0.07239205855625692, 0.07220566778929836, 0.07202009178076858, 0.07183532643375552, 0.0716514848888931, 0.07146833616853021, 0.0712860051705155, 0.07110459169663665, 0.0709239677815439, 0.07074413167965843, 0.07056508140060874, 0.07038685282270953, 0.07020956353551137, 0.07003294706873883, 0.06985712636352774, 0.06968220581378881, 0.06950804942941012, 0.0693346329902383, 0.06916194516083675, 0.06898998267412162, 0.06881881938088685, 0.0686485091942978, 0.06847883179411865, 0.06830969220630859, 0.06814130741590317, 0.06797347652650772, 0.0678063367788976, 0.06763987485407454, 0.06747421935601239, 0.06730935686255252, 0.06714514312627261, 0.066981628270235, 0.0668189165164091, 0.06665682737931285, 0.06649521420157758, 0.06633441003605495, 0.06617428301898523, 0.06601481579698852, 0.06585605304238908, 0.06569791452964097, 0.06554008350054714, 0.06538250879030601, 0.06522558827699759, 0.06506927197423323, 0.06491359782738734, 0.06475844488325896, 0.06460392687405263, 0.06445001698060893, 0.06429673196771633, 0.06414397955915177, 0.0639915721256364, 0.06384008554980233, 0.06368920619642382, 0.06353882568174839, 0.06338905776501363, 0.06323986041243501, 0.06309131941241478, 0.06294323200879638, 0.06279584572484226, 0.0626490136750333, 0.06250276354652523, 0.06235706404181146, 0.06221191530555015, 0.06206731110064628, 0.06192326382364107, 0.061779880214284, 0.0616374461749817, 0.06149545688322648, 0.06135390760572042, 0.06121296902269058, 0.06107272344268981, 0.06093303285130467, 0.06079387313828605, 0.06065526582671987, 0.06051720251822349, 0.060379654092138024, 0.06024247315182567, 0.06010600427577038, 0.05996950195220167, 0.05983151581144334, 0.05969331380165862, 0.05955482157288555, 0.05941526672817328, 0.05927559574215106, 0.05913585675602242, 0.058996136424786724, 0.05885653309606918, 0.05871732508892309, 0.058578388630816526, 0.05843973902326618, 0.058301420192222356, 0.058163476461419505, 0.05802595337876095, 0.05788886066381689, 0.05775223682603253, 0.05761607118687586, 0.057480340990847666, 0.057345138355842026, 0.057210452894617604, 0.05707629479140698, 0.056942673451542615, 0.05680960512382257, 0.05667709979313893, 0.05654514222681493, 0.0564137543163599, 0.05628256323685234, 0.05615132261475051, 0.05602056689720795, 0.05589029337533337, 0.05576054124661478, 0.05563141411397364, 0.05550270422205437, 0.055374020369818824, 0.05524577068466639, 0.055118022424066306, 0.05499076383679913, 0.05486396814630281, 0.05473764743070033, 0.05461206329532707, 0.05448743834071459, 0.054363321175710175, 0.05423970502950071, 0.054116600372019354, 0.05399400319889551, 0.05387189684107021, 0.05375029486776001, 0.053629192843568575, 0.05350857066757109, 0.05338844065649036, 0.05326879432664542, 0.05314962134291081, 0.05303092838794545, 0.052912689000010685, 0.052794667037548516, 0.05267709556484679, 0.05255997245250623, 0.052444686253371856, 0.052331301170458075, 0.05221827113206183, 0.05210559549780023, 0.051993285866193985, 0.0518813481259928, 0.051769778053587755, 0.05165858839843853, 0.05154778320412891, 0.0514373547752925, 0.051327317411165994, 0.05121766899663814, 0.051108405884734955, 0.05099954274543269, 0.05089107326231944, 0.05078299001056176, 0.050673250380925725, 0.05056326429977133, 0.050453385840729205, 0.05034364415475591, 0.05023406910378046, 0.05012470950578907, 0.050015538539864325, 0.04990918589349847, 0.04980672994880911, 0.049704771276989114, 0.04960529166337034, 0.04950633895562459, 0.04940784881102317, 0.049309811343387466, 0.04921224139295092, 0.04911510450472331, 0.04901838911958255, 0.04892210744408763, 0.04882629264770942, 0.0487308896144312, 0.048635888650362814]\n"
     ]
    }
   ],
   "source": [
    "loss = mlp.loss_curve_ \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7272727272727273\n",
      "Recall: 1.0\n",
      "F1-score: 0.8421052631578948\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris-setosa Iris-virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching iris dataset\n",
      "(150, 4) (150,)\n",
      "Dataset:  iris\n",
      "X_train:  [[ 1.91690205 -0.47344653  1.48387657  1.05511751]\n",
      " [ 0.21582425 -0.47344653  0.62004157  0.7271017 ]]\n",
      "y_train:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "X_test:  [[ 0.53477633 -0.95655523  0.76401407  0.3990859 ]\n",
      " [ 0.53477633 -0.71500088  1.00396823  0.7271017 ]]\n",
      "y_test:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "(80, 4) (80, 2) (20, 4) (20, 2)\n",
      "Exporting iris to python... \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(15,), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = Dataloader(None, None, 'iris', 'Iris-setosa', 'Iris-virginica')\n",
    "print(f\"Exporting iris to python... \")\n",
    "data = loader.export_to_python()\n",
    "X_shape = loader.X_train_shape\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "y = array_to_matrix(y_train,2)\n",
    "X = array_to_matrix(X_train,X_shape[1])\n",
    "y = np.array([np.argmax(np.append(i,0)) for i in y])\n",
    "mlp.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8011223984289269, 0.7936508097956056, 0.7862390559996726, 0.7788968353817244, 0.7716312478984348, 0.7644335294599075, 0.7572986332056824, 0.7502325425349746, 0.7432393186536258, 0.7363127880761023, 0.7294492294333514, 0.7226519539760682, 0.7159265368255399, 0.7092711346552186, 0.7026896694403069, 0.6961710612172013, 0.6897201524568204, 0.6833570022358151, 0.677067032040697, 0.6708404383285338, 0.664672734352124, 0.6585727429721718, 0.6525343094475105, 0.646549619277752, 0.6406231766199223, 0.6347632459582757, 0.6289670962566314, 0.623230502777622, 0.6175646798800855, 0.6119695057130695, 0.6064360808808203, 0.6009623842136498, 0.5955459606418164, 0.5901899201734132, 0.5848854418227557, 0.5796319393482505, 0.5744385327876246, 0.5693029325926526, 0.5642205287417518, 0.5592015176271323, 0.554238854126738, 0.549321334309941, 0.5444464922908716, 0.5396078957600846, 0.5348056437013795, 0.5300418448861048, 0.5253230242190404, 0.5206548492761631, 0.5160339956791609, 0.5114645649390666, 0.5069401296746512, 0.5024529353242384, 0.49799574965031396, 0.49357813653771043, 0.48920343937163907, 0.48487309802263623, 0.4805903648328707, 0.4763545530498354, 0.4721634511560356, 0.468017222494463, 0.46390815519072603, 0.45983320860398597, 0.45579925277255606, 0.4518016735123725, 0.447844192636612, 0.44392299954159753, 0.4400354466136133, 0.4361836412992143, 0.4323656934325923, 0.42857653744555274, 0.4248219617804737, 0.42110622675549403, 0.41742328394688927, 0.41377523680875566, 0.41016334894677575, 0.40658351451172264, 0.4030365654957543, 0.3995155059814757, 0.39602473419438783, 0.3925635482210987, 0.3891333546289063, 0.3857317675972027, 0.3823518551595111, 0.3789936170924933, 0.3756526837722334, 0.37233796940459785, 0.3690496971136092, 0.36578497743762767, 0.36254743667350264, 0.3593396876929384, 0.3561639713607847, 0.3530149419999508, 0.3498894545545114, 0.34678970962926897, 0.34371468582664455, 0.34066582656307703, 0.33764437453379903, 0.3346508330193925, 0.33168216645416804, 0.3287401219672173, 0.32582345367937054, 0.3229273719920824, 0.32005307367615826, 0.3172013243530991, 0.3143737016420433, 0.3115705910385084, 0.30879119643628505, 0.3060354795651759, 0.30329895406719276, 0.3005802739171502, 0.29788125730728865, 0.29519369130712014, 0.2925220203161613, 0.289870027084954, 0.2872362466026361, 0.28462424447421963, 0.282032155782965, 0.27945450097656815, 0.27689759062798874, 0.27436321308335243, 0.2718507681381162, 0.26936121021254145, 0.266893445244255, 0.26444751302554875, 0.2620244231761335, 0.25962384558494545, 0.2572460465951997, 0.25489020137179896, 0.2525562441864963, 0.2502423891339137, 0.24794774660499516, 0.24567471855566195, 0.2434240346850088, 0.2411946676351938, 0.23898676785113496, 0.23680288302347416, 0.23464033054783556, 0.23249783343114103, 0.23037604521189628, 0.22827530972567053, 0.22619354384652485, 0.2241285614511221, 0.22208252257583125, 0.22005626269354633, 0.21805004592292887, 0.21606557401988075, 0.21410104950832806, 0.21215090584703594, 0.21021873572144445, 0.20830052805740604, 0.20640074913903833, 0.20451893192090748, 0.20265328047830727, 0.20080581716425563, 0.19897682175531053, 0.19716593820088055, 0.19537290996612894, 0.19359527067348647, 0.19183517753737664, 0.19009256203794042, 0.1883667688054389, 0.186658387681347, 0.18496473891410578, 0.18328683116164368, 0.18162596165981365, 0.17998311842397843, 0.17835708641323073, 0.17674777369779507, 0.17515508068459512, 0.17357522922162197, 0.17200986952865224, 0.1704606894232227, 0.16892766970603446, 0.16741109291645265, 0.16590991621459866, 0.16442444819205945, 0.16295450867459488, 0.16149998802717383, 0.16006077160699955, 0.15863674041495848, 0.157227771679298, 0.15583283646371923, 0.1544527879422117, 0.1530887200375282, 0.15173922289081382, 0.15040415938509635, 0.14908339117102443, 0.14777688119489568, 0.14648508010608788, 0.1452064150316322, 0.14393970196184397, 0.14268641199018245, 0.1414464294839779, 0.1402196362184485, 0.1390059117750438, 0.13780513389987425, 0.13661717882611255, 0.13544192156388143, 0.1342792361608046, 0.13312899593608854, 0.13199126223391222, 0.13086616107379814, 0.12975314965547763, 0.1286521262569677, 0.12756370504057105, 0.12648739723259417, 0.12542270358325883, 0.12436948565951793, 0.12332760685068433, 0.12229693231840208, 0.12127770927196249, 0.12026966695150905, 0.11927284161960705, 0.11828681864454058, 0.11731140281383814, 0.1163464611223786, 0.11539214754513587, 0.11444820391162373, 0.11351436606599438, 0.11259050637268594, 0.11167627406945338, 0.110771501973176, 0.10987628545048603, 0.108990512414568, 0.10811407199037115, 0.10724694905768702, 0.10638953864902576, 0.10554144593637313, 0.10470230057386587, 0.10387199063175653, 0.10305040605657215, 0.10223743859910642, 0.10143298174806373, 0.10063678700403385, 0.09984848085259872, 0.09906834235219529, 0.09829627695614242, 0.09753219098837589, 0.09677599166223316, 0.09602758709617046, 0.0952869165057039, 0.09455409965729626, 0.09382988713326135, 0.09311342224952493, 0.0924045408971783, 0.09170333567771563, 0.09100938322406729, 0.09032235187914961, 0.08964176142117859, 0.08896808468588277, 0.0883012385956546, 0.08764114119726454, 0.08698771164146729, 0.08634087016337842, 0.0857005380635731, 0.08506663768986064, 0.08443909241969359, 0.08381782664317106, 0.08320276574660021, 0.08259383609658132, 0.08199096502458457, 0.08139408081198865, 0.08080193285633255, 0.08021468748661655, 0.07963302649365694, 0.07905690957690757, 0.07848629436490219, 0.07792113670174382, 0.07736139090396928, 0.07680700999073053, 0.07625794588994955, 0.07571414962284548, 0.07517557146899789, 0.07464216111389796, 0.07411386778074706, 0.07359064034808849, 0.07307242745470038, 0.07255917759303708, 0.07205083919237663, 0.07154736069271819, 0.07104839307498481, 0.07055381469885669, 0.07006389553509666, 0.06957874567394465, 0.06909818431354986, 0.06862196921120475, 0.06815016917909963, 0.0676827979683347, 0.06721981211788185, 0.06676116790912061, 0.06630682145233545, 0.065856910108772, 0.0654113072286578, 0.064969885813047, 0.0645325993987982, 0.06409940193367195, 0.06367030341188319, 0.06324543747775424, 0.06282484087619411, 0.06240816632998779, 0.06199520406990231, 0.06158607532801903, 0.06118073540933834, 0.060779160904292494, 0.06038146275226662, 0.05998743529717963, 0.059597034666374823, 0.0592102177578148, 0.05882712264101847, 0.05844781755224147, 0.05807198985079172, 0.057699596736290756, 0.05733059607940185, 0.0569649466214681, 0.05660260793175721, 0.05624354036822578, 0.055887737687787556, 0.05553546171131803, 0.05518636039741791, 0.05484039432572848, 0.05449752497597986, 0.054157714677658245, 0.05382092656407624, 0.053487124530419025, 0.053156273195380375, 0.052828337866042616, 0.052503284505688884, 0.052181079704266754, 0.05186169065125022, 0.05154508511067302, 0.05123123139812684, 0.050919861150554115, 0.05061112591135147, 0.05030503083307256, 0.05000154910187976, 0.04970067211021842, 0.049403073605083274, 0.04910795186653815, 0.0488152866975031, 0.04852500823457212, 0.048236856043005774, 0.0479510733597545, 0.04766764228751671, 0.047386544422948124, 0.04710776093405249, 0.04683127262973481, 0.04655706002225811, 0.04628520581471102, 0.04601567663894411, 0.04574837099272182, 0.04548326738556891, 0.045220344399090874, 0.04495958070498455, 0.044700955080942406, 0.04444483916594925, 0.04419087418601543, 0.043938937408613814, 0.04368901217756255, 0.04344108163270391, 0.04319512874985644, 0.042951136376989225, 0.042709185655329523, 0.042469275114826704, 0.04223167230980969, 0.04199597243814746, 0.041762151842474504, 0.04153019199079341, 0.04130007451327713, 0.041071781205765295, 0.04084531671228197, 0.04062066353538028, 0.04039778294118248, 0.04017665707011274, 0.03995726827532495, 0.039739795489092115, 0.03952402125394158, 0.03930988851405267, 0.03909738473106183, 0.03888649716045592, 0.03867723585664495, 0.038469695443808224, 0.038263878698701, 0.038059631993939726, 0.0378569411186574, 0.03765579190927845, 0.037456170260182456, 0.03725806213318206, 0.03706191638510709, 0.03686757026935261, 0.03667465549725019, 0.036483161668385375, 0.036293220594083624, 0.03610468946048355, 0.03591755522580049, 0.03573180604219186, 0.03554743004340469, 0.035364415359801255, 0.035182726839292786, 0.03500234096877569, 0.03482323682824331, 0.03464543235261539, 0.03446897397722327, 0.034293788862207665, 0.034119867557577004, 0.03394728535290464, 0.033775916345183676, 0.0336057518828037, 0.033436858601916634, 0.033269209895191665, 0.03310278566331592, 0.0329375842521446, 0.03277356307056937, 0.032610753418185964, 0.03244912754315023, 0.032288621357777554, 0.0321293087351571, 0.03197113825575042, 0.03181407915200312, 0.0316581227850189, 0.031503260458187146, 0.03134954371819675, 0.031196908493403515, 0.031045309695504973, 0.030894791748350588, 0.03074536069772731, 0.030596961083643254, 0.030449676820266162, 0.030303532427605008, 0.030158499095786374, 0.030014450026669654, 0.029871371795606526, 0.02972921920529057, 0.02958803026463753, 0.02944779549181368, 0.029308559268761948, 0.029170536233883188, 0.029033477943384713, 0.02889738253636443, 0.028762223890859633, 0.028627980764863843, 0.028494645087896887, 0.02836228539942983, 0.02823078909982963, 0.02810010422767864, 0.027970287629005497, 0.027841402559012123, 0.027713366712968625, 0.027586173910297905, 0.027459817921895835, 0.027334316152527384, 0.02720966417574361, 0.027085806380067756, 0.026962792265754997, 0.026840594765142823, 0.026719189291511423, 0.026598585437433354, 0.02647878861930203, 0.026359746813115308, 0.02624151962172141, 0.026124053272623647, 0.02600733740309131, 0.025891361446763527, 0.025776119733544163, 0.025661686103000673, 0.02554796267008218, 0.025434934002504204, 0.025322621752416348, 0.02521107608594216, 0.025100230365985122, 0.024990080130704626, 0.02488062083213938, 0.02477201695222419, 0.024664087017114245, 0.02455681323237192, 0.024450232099446068, 0.024344393713562347, 0.02423925930227624, 0.024134779940213017, 0.02403096756411734, 0.02392786682333707, 0.023825417777622517, 0.02372357976597392, 0.023622437440388114, 0.023521917074686087, 0.02342201143285082, 0.023322716182919777, 0.02322402697702459, 0.0231259394570327, 0.023028509071640595, 0.022931669760354264, 0.022835390730782848, 0.02273968001386174, 0.022644625772800363, 0.022550159743917646, 0.022456260997511218]\n"
     ]
    }
   ],
   "source": [
    "loss = mlp.loss_curve_ \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_test = array_to_matrix(data[2],X_shape[1])\n",
    "y_test = array_to_matrix(data[3],2)\n",
    "y_test = np.array([np.argmax(np.append(i,0)) for i in y_test])\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mushroom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching mushroom dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:  mushroom\n",
      "X_train:  [[-0.21699152  0.14012794 -0.98389939 -0.84322964  1.3573133   0.16289645\n",
      "  -0.43886364  1.49468272 -1.35889624  0.87351064  1.35578135  0.68377765\n",
      "  -0.89305291  0.0965768   0.63199138  0.          0.14203663 -0.25613174\n",
      "  -1.27221574  1.42842641  0.28432981 -0.8771691 ]\n",
      " [ 1.02971224  0.14012794 -0.19824983 -0.84322964 -1.01956488  0.16289645\n",
      "  -0.43886364  1.49468272 -1.35889624  0.87351064  1.35578135 -0.9254372\n",
      "   0.58638466  0.62244139  0.63199138  0.          0.14203663 -0.25613174\n",
      "  -1.27221574  1.42842641  0.28432981  1.44858865]]\n",
      "y_train:  [[-1.  1.]\n",
      " [-1.  1.]]\n",
      "X_test:  [[-0.8403434  -1.48615695 -0.19824983 -0.84322964  0.40656203  0.16289645\n",
      "   2.27861212 -0.66903831 -0.51147238  0.87351064  0.20869036  0.68377765\n",
      "  -2.37249048  0.62244139  0.63199138  0.          0.14203663 -0.25613174\n",
      "  -1.27221574 -0.2504706  -0.5143892  -0.29572966]\n",
      " [-0.8403434   0.14012794 -0.98389939 -0.84322964  1.83268894  0.16289645\n",
      "  -0.43886364  1.49468272 -1.35889624  0.87351064  1.35578135  0.68377765\n",
      "   0.58638466  0.0965768   0.10765539  0.          0.14203663 -0.25613174\n",
      "  -1.27221574  1.42842641  0.28432981  0.28570978]]\n",
      "y_test:  [[ 1. -1.]\n",
      " [-1.  1.]]\n",
      "(6499, 22) (6499, 2) (1625, 22) (1625, 2)\n",
      "Exporting mushroom to python... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n",
      "c:\\Users\\ACER\\Documents\\AI\\KDL\\KDL\\utils.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.loc[:, col] = le.fit_transform(X[col])\n"
     ]
    }
   ],
   "source": [
    "mushroom = Dataloader(None, None, 'mushroom', None, None)\n",
    "print(f\"Exporting mushroom to python... \")\n",
    "data = mushroom.export_to_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp_mushroom = MLPClassifier(hidden_layer_sizes=(150, 100, 2), activation='relu', solver='adam', random_state=1, max_iter=1000)\n",
    "\n",
    "\n",
    "X_shape = mushroom.X_train_shape\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "y = array_to_matrix(y_train,2)\n",
    "X = array_to_matrix(X_train,X_shape[1])\n",
    "y = np.array([np.argmax(np.append(i,0)) for i in y])\n",
    "mlp_mushroom.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4367013282738765, 0.2973056388936893, 0.26214814134595077, 0.24617357490900743, 0.23579137989946275, 0.22763444625424786, 0.22074724887252536, 0.21408365756114767, 0.20802952815104397, 0.20241122220205351, 0.1969549025742592, 0.19178520816863712, 0.18679946212211498, 0.18206480696467436, 0.17743505791025177, 0.17317221130922197, 0.16876069944247854, 0.1646761069988514, 0.16066453982559287, 0.15677866802985727, 0.1530375816899188, 0.14941262083443008, 0.14591682059407515, 0.14250895912090605, 0.1391739256932269, 0.1359861773599788, 0.1328935014071939, 0.12988778348828292, 0.12694243853221837, 0.12409136502307067, 0.12131313404456619, 0.1186231297138705, 0.11601055938472177, 0.1134951450714109, 0.11107289230828779, 0.1086342288397094, 0.10628614378556774, 0.10401538551528548, 0.10180700011217235, 0.09967967281836718, 0.09757866612506677, 0.09555728731421795, 0.09358328431030434, 0.09165476371410149, 0.08977189325853371, 0.08794018260834006, 0.08616282574610883, 0.08443762439628341, 0.08273360167169945, 0.08108311736448695, 0.0794746429203056, 0.07791055233027802, 0.0763814131968938, 0.0749116972369647, 0.07345226477338593, 0.07202354980511552, 0.07063793443136014, 0.06929045170973194, 0.0679708199187716, 0.066684502672466, 0.06543548339997271, 0.06420005378710877, 0.06300442381832067, 0.061835075428338236, 0.06068627915695686, 0.05958367313776474, 0.05847764815710187, 0.057413678796965975, 0.056369136270735296, 0.055345862863848724, 0.054352043565757506, 0.05340200403498309, 0.05242168577497509, 0.05148711477578921, 0.050571585947059274, 0.049684580123538116, 0.04880730261728992, 0.04794964810792774, 0.047109962646864935, 0.0462908686152959, 0.045487442470555244, 0.04470827729517453, 0.043932331288006776, 0.04317921493814278, 0.04244420169472974, 0.041717521440775944, 0.04101118477376451, 0.040316936129040956, 0.03963588343222901, 0.03896930512858758, 0.038320171676254955, 0.03767833710879804, 0.03704809864002286, 0.036435622505574035, 0.035835230244215595, 0.035235775363350735, 0.03466181328619647, 0.034095617152900014, 0.033530149069072784, 0.03298349544806154, 0.03244854460366283, 0.03192125427370545, 0.03140537047143071, 0.030898388812587496, 0.030400607031242054, 0.029911815273761536, 0.029434398344575913, 0.02896392704350673, 0.02850365714720842, 0.02804993325204096, 0.02760723701738915, 0.02717110330467328, 0.026750450214664285, 0.026323223186903597, 0.025907319479164914, 0.025501077958809435, 0.025103184114784932, 0.024710432597813317, 0.024328613500139575, 0.023948949405976867, 0.02357575675866768, 0.02321131149255802, 0.022852893852501013, 0.022499879498560707, 0.022153481016066477, 0.021813451028897174, 0.021479887528087424, 0.02115271962685382, 0.02082912670704913, 0.020510323923204534, 0.02019796898792129, 0.019890914574688296, 0.01959051172764791, 0.01929359467417563, 0.019001587582629813, 0.018722638564994395, 0.018433762249820466, 0.01816153711783091, 0.017886456791312497, 0.01761746723499437, 0.017352573559201764, 0.017094030031059106, 0.016838851600053502, 0.01659021308238967, 0.016342464831001966, 0.016099518516189015, 0.0158616598947024, 0.01562737816122157, 0.015396113259358801, 0.01516950401734275, 0.01494559400915586, 0.014725576156510193, 0.014508941184102548, 0.014296482601021263, 0.014086844667096306, 0.013880667641864053, 0.013676972341295262, 0.013477813149401911, 0.013280875879503646, 0.013087522306864888, 0.012897757917393744, 0.012710674792876896, 0.012525918208103757, 0.012344650861522161, 0.012167138622000912, 0.01198930365104627, 0.011816408558844149, 0.011645040276279904, 0.011477269720054207, 0.011312186519444089, 0.011149596921682277, 0.010994550432879403, 0.010831756315297496, 0.010677944557709412, 0.010522262770965204, 0.01037213201021123, 0.010223946202582596, 0.010076748928004102, 0.009933376763557917, 0.009791285909703752, 0.009653681941934642, 0.009514179195509958, 0.009378366929081653, 0.00924618230313134, 0.009116482997967463, 0.008984212164177894, 0.008858435079477199, 0.008732756398449967, 0.008607536449367539, 0.008485090462072117, 0.008365099391351404, 0.008246664866394601, 0.008130521018054043, 0.008015542729461174, 0.00790344543281966, 0.007791056855521329, 0.0076807251085792, 0.007572485472867235, 0.0074656152518151764, 0.0073605135552756735, 0.0072569461539707565, 0.007154610073200871, 0.007054225138847007, 0.006955007667228449, 0.006858825537167669, 0.006761708984130921, 0.006666986195033977, 0.006573887122064604, 0.00648201782056444, 0.006391102359345581, 0.006301863311425609, 0.0062138758282158915, 0.0061270274738488475, 0.006042107695654007]\n"
     ]
    }
   ],
   "source": [
    "loss = mlp_mushroom.loss_curve_ \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "(22, 150)\n",
      "Layer 1:\n",
      "(150, 100)\n",
      "Layer 2:\n",
      "(100, 2)\n",
      "Layer 3:\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách các lớp của mạng nơ-ron\n",
    "layers = mlp_mushroom.coefs_\n",
    "\n",
    "# In các trọng số của từng lớp\n",
    "for i, layer in enumerate(layers):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(layer.shape)\n",
    "    # print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_test = array_to_matrix(data[2],X_shape[1])\n",
    "y_test = array_to_matrix(data[3],2)\n",
    "y_test = np.array([np.argmax(np.append(i,0)) for i in y_test])\n",
    "\n",
    "y_pred = mlp_mushroom.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1-score: 1.0\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 1 0 0 1 1]\n",
      "Dataset:  mnist\n",
      "X_train:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "y_train:  [[ 1. -1.]\n",
      " [-1.  1.]]\n",
      "X_test:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "y_test:  [[-1.  1.]\n",
      " [ 1. -1.]]\n",
      "(12665, 784) (12665, 2) (2115, 784) (2115, 2)\n",
      "Exporting mnist to python... \n"
     ]
    }
   ],
   "source": [
    "from utils import Dataloader\n",
    "mnist = Dataloader(None, None, 'mnist', None, None)\n",
    "print(f\"Exporting mnist to python... \")\n",
    "data = mnist.export_to_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(150, 100, 2), max_iter=1000, random_state=1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mlp_mnist = MLPClassifier(hidden_layer_sizes=(150, 100, 2), activation='relu', solver='adam', random_state=1, max_iter=1000)\n",
    "X_shape = mnist.X_train_shape\n",
    "X_train = data[0]\n",
    "y_train = data[1]\n",
    "y = array_to_matrix(y_train,2)\n",
    "X = array_to_matrix(X_train,X_shape[1])\n",
    "y = np.array([np.argmax(np.append(i,0)) for i in y])\n",
    "mlp_mnist.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10095602329130694, 0.003409001912717198, 0.0015081029886822286, 0.0007494766730253629, 0.00048818559342137614, 0.0003989386825165058, 0.0003085814465386173, 0.00023359271825939507, 0.00019602351563095824, 0.00017238757465550298, 0.0001586252266479318, 0.00014749153698184327, 0.00013599626665210598, 0.00013003355171036063, 0.00012393263350977845, 0.00011823713781561198]\n"
     ]
    }
   ],
   "source": [
    "loss = mlp_mnist.loss_curve_ \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:\n",
      "(784, 150)\n",
      "[[ 3.91309861e-21  5.76246092e-18 -3.87403397e-11 ...  3.19981792e-18\n",
      "  -6.53697284e-22 -2.42403995e-11]\n",
      " [-1.86861611e-12  1.08436224e-11 -1.92265929e-21 ... -1.27838657e-12\n",
      "   2.02575377e-11  5.26972239e-16]\n",
      " [-6.82955713e-16  9.21234926e-14 -4.47223808e-19 ...  9.84631575e-13\n",
      "  -1.94359435e-14 -5.90251289e-15]\n",
      " ...\n",
      " [ 3.51671289e-20  8.38454387e-16  1.46438225e-13 ...  7.64083050e-12\n",
      "   3.32927433e-20 -5.73784748e-12]\n",
      " [ 3.64332733e-11  5.10915708e-16 -9.17552161e-18 ... -4.43112577e-19\n",
      "  -3.76426339e-14 -5.31338631e-19]\n",
      " [ 5.07575801e-14  5.28095137e-19  9.80753283e-14 ... -3.40144852e-12\n",
      "   4.98216331e-19 -1.55943368e-13]]\n",
      "Layer 1:\n",
      "(150, 100)\n",
      "[[-0.12986352 -0.13198536 -0.06068928 ... -0.09453962 -0.07131752\n",
      "  -0.08460738]\n",
      " [ 0.03816402 -0.09811022  0.05912366 ...  0.14096911  0.01705887\n",
      "  -0.1128716 ]\n",
      " [ 0.11692913  0.09388404  0.0521386  ...  0.15213696  0.0313308\n",
      "   0.07074859]\n",
      " ...\n",
      " [-0.11332766 -0.07210476  0.04858068 ... -0.0090965  -0.08788963\n",
      "   0.10100718]\n",
      " [-0.09349709 -0.08820048  0.03345418 ... -0.01264974  0.08966761\n",
      "   0.03168515]\n",
      " [-0.07540856 -0.05402035 -0.1184925  ...  0.1310808   0.1229631\n",
      "   0.09047743]]\n",
      "Layer 2:\n",
      "(100, 2)\n",
      "[[-0.16369274 -0.04583788]\n",
      " [ 0.08837849 -0.02173514]\n",
      " [-0.22408892  0.10559412]\n",
      " [ 0.20574669  0.08094141]\n",
      " [ 0.10269667  0.04866746]\n",
      " [-0.16853594  0.16962141]\n",
      " [-0.11798817  0.18344192]\n",
      " [-0.03195156 -0.07853   ]\n",
      " [-0.0178305   0.19803441]\n",
      " [-0.22586065  0.00410125]\n",
      " [ 0.05295149 -0.03494439]\n",
      " [ 0.0822217   0.21091136]\n",
      " [ 0.13869854 -0.22272535]\n",
      " [ 0.13911791 -0.16847855]\n",
      " [-0.14751389  0.1010634 ]\n",
      " [ 0.02314421  0.15699986]\n",
      " [-0.22414396  0.18265064]\n",
      " [ 0.1206237   0.18332499]\n",
      " [ 0.07425225 -0.01417176]\n",
      " [ 0.19491826  0.13255975]\n",
      " [ 0.25524368  0.21614025]\n",
      " [ 0.19556891 -0.20965298]\n",
      " [-0.09283372 -0.21725862]\n",
      " [-0.16060399 -0.18224795]\n",
      " [-0.19446371 -0.08402696]\n",
      " [-0.15038194  0.2496164 ]\n",
      " [ 0.13811459  0.12659052]\n",
      " [-0.08354261  0.0662735 ]\n",
      " [ 0.03483706  0.22272554]\n",
      " [-0.20799844 -0.06349981]\n",
      " [-0.16855713  0.00170004]\n",
      " [-0.06286887 -0.04207208]\n",
      " [-0.08261645 -0.05503314]\n",
      " [ 0.17061537 -0.11110516]\n",
      " [-0.01319569  0.19901734]\n",
      " [-0.19196624 -0.21100862]\n",
      " [-0.03584493  0.2197546 ]\n",
      " [ 0.27763034 -0.1066864 ]\n",
      " [-0.20339155 -0.02432523]\n",
      " [ 0.03961624  0.05940809]\n",
      " [ 0.07818328 -0.0887589 ]\n",
      " [-0.17082327  0.0750863 ]\n",
      " [ 0.04782287  0.06954789]\n",
      " [-0.02970967 -0.04886506]\n",
      " [-0.15275945 -0.17156478]\n",
      " [ 0.29461672  0.09996552]\n",
      " [ 0.28428348 -0.19467092]\n",
      " [-0.16470655 -0.20086677]\n",
      " [-0.17287546  0.22679743]\n",
      " [-0.13608904  0.04948126]\n",
      " [-0.18427044  0.21689151]\n",
      " [-0.02253028 -0.00741795]\n",
      " [ 0.17295179 -0.12409427]\n",
      " [-0.1530933  -0.02682981]\n",
      " [ 0.23611396  0.16104487]\n",
      " [ 0.27794134  0.20770448]\n",
      " [-0.03063227 -0.22268704]\n",
      " [-0.06104119  0.09965092]\n",
      " [ 0.1393633  -0.03764607]\n",
      " [-0.2121849   0.20030028]\n",
      " [ 0.06384203  0.01142031]\n",
      " [-0.09426608  0.24815823]\n",
      " [-0.0055889  -0.04501729]\n",
      " [-0.2191734   0.09206716]\n",
      " [ 0.1897929  -0.04369059]\n",
      " [ 0.2207344  -0.00723224]\n",
      " [-0.07778389 -0.09149504]\n",
      " [ 0.21176379 -0.06756568]\n",
      " [ 0.15209264 -0.16533725]\n",
      " [ 0.19739469 -0.10525055]\n",
      " [-0.00920809 -0.13310896]\n",
      " [-0.04821585  0.25378564]\n",
      " [ 0.15630173 -0.08913239]\n",
      " [ 0.03367022 -0.20485879]\n",
      " [ 0.28504726 -0.21630333]\n",
      " [ 0.03881136 -0.11933296]\n",
      " [-0.20071077  0.23620363]\n",
      " [-0.15538725  0.17852273]\n",
      " [ 0.04287494 -0.03924612]\n",
      " [ 0.03799298  0.0748452 ]\n",
      " [-0.19697636 -0.04959463]\n",
      " [ 0.02829945 -0.21627782]\n",
      " [ 0.17579439 -0.1750256 ]\n",
      " [-0.01642914 -0.2690624 ]\n",
      " [ 0.09582068 -0.18698124]\n",
      " [ 0.26625065 -0.00132619]\n",
      " [ 0.20693374 -0.03950303]\n",
      " [-0.01233346 -0.05815029]\n",
      " [-0.17060883  0.25247116]\n",
      " [-0.22377248  0.15017649]\n",
      " [ 0.20127765  0.2040255 ]\n",
      " [ 0.02212289  0.00977841]\n",
      " [-0.06268078  0.13462583]\n",
      " [-0.07045162 -0.18341827]\n",
      " [-0.05339089  0.05814637]\n",
      " [-0.20336005 -0.14891416]\n",
      " [-0.24731109  0.13422836]\n",
      " [ 0.10300405  0.17939013]\n",
      " [-0.20637323 -0.17613615]\n",
      " [ 0.19202551 -0.10849281]]\n",
      "Layer 3:\n",
      "(2, 1)\n",
      "[[ 0.28629638]\n",
      " [-0.78849279]]\n"
     ]
    }
   ],
   "source": [
    "# Lấy danh sách các lớp của mạng nơ-ron\n",
    "layers = mlp_mnist.coefs_\n",
    "\n",
    "# In các trọng số của từng lớp\n",
    "for i, layer in enumerate(layers):\n",
    "    print(f\"Layer {i}:\")\n",
    "    print(layer.shape)\n",
    "    print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12665, 784)\n",
      "[1 0 1 ... 1 0 1]\n",
      "Accuracy: 0.9985815602836879\n"
     ]
    }
   ],
   "source": [
    "X_test = array_to_matrix(data[2],X_shape[1])\n",
    "y_test = array_to_matrix(data[3],2)\n",
    "y_test = np.array([np.argmax(np.append(i,0)) for i in y_test])\n",
    "print(X_shape)\n",
    "y_pred = mlp_mnist.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9973637961335676\n",
      "Recall: 1.0\n",
      "F1-score: 0.9986801583809943\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
